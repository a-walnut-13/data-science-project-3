{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Math 5750/6880: Mathematics of Data Science \\\n",
        "Project 3"
      ],
      "metadata": {
        "id": "0gdC70xxFyc4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Fashion-MNIST image classification using sklearn"
      ],
      "metadata": {
        "id": "i9_7SnpMGKDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load Fashion-MNIST\n",
        "# Classes (0-9): T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "X_train = X_train.reshape(len(X_train), -1)\n",
        "X_test  = X_test.reshape(len(X_test), -1)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "AB136H0PGKq1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2acf4e58-59a5-4be7-9b05-ed2f5dedb994"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import time\n",
        "\n",
        "# Base Model\n",
        "# mlp_baseline = MLPClassifier(hidden_layer_sizes=(100,),\n",
        "#                              activation='logistic',\n",
        "#                              solver='adam',\n",
        "#                              learning_rate_init=0.001, random_state=42,\n",
        "#                              verbose=True, early_stopping=True)\n",
        "\n",
        "# Test Model (two hidden layers)\n",
        "mlp_baseline = MLPClassifier(hidden_layer_sizes=(256, 128),\n",
        "                             activation='relu',\n",
        "                             learning_rate='adaptive',\n",
        "                             learning_rate_init=0.001,\n",
        "                             momentum=0.9,\n",
        "                             early_stopping=True,\n",
        "                             random_state=42,\n",
        "                             verbose=True)\n",
        "\n",
        "# Test Model (three hidden layers)\n",
        "# mlp_baseline = MLPClassifier(hidden_layer_sizes=(512, 256, 128),\n",
        "#                              activation='logistic',\n",
        "#                              solver='adam',\n",
        "#                              learning_rate_init=0.001,\n",
        "#                              early_stopping=True,\n",
        "#                              random_state=42,\n",
        "#                              verbose=True)\n",
        "\n",
        "start_time = time.time()\n",
        "mlp_baseline.fit(X_train, y_train)\n",
        "train_time = time.time() - start_time\n",
        "\n",
        "# Evaluate\n",
        "y_predict = mlp_baseline.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_predict)\n",
        "cm = confusion_matrix(y_test, y_predict)\n",
        "\n",
        "print(f\"Baseline Accuracy: {acc:.4f}\")\n",
        "print(f\"Training Time: {train_time:.2f} seconds\")\n",
        "print(\"Confusion Matrix:\\n\", cm)"
      ],
      "metadata": {
        "id": "5GAsN-dmHjRM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1918fe78-60ff-4f4d-993c-37cb939910a7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 0.45783715\n",
            "Validation score: 0.872000\n",
            "Iteration 2, loss = 0.32062740\n",
            "Validation score: 0.875167\n",
            "Iteration 3, loss = 0.27955216\n",
            "Validation score: 0.877500\n",
            "Iteration 4, loss = 0.24878781\n",
            "Validation score: 0.882500\n",
            "Iteration 5, loss = 0.22653632\n",
            "Validation score: 0.888500\n",
            "Iteration 6, loss = 0.20629701\n",
            "Validation score: 0.890333\n",
            "Iteration 7, loss = 0.19099525\n",
            "Validation score: 0.887500\n",
            "Iteration 8, loss = 0.17115681\n",
            "Validation score: 0.887833\n",
            "Iteration 9, loss = 0.15745049\n",
            "Validation score: 0.892833\n",
            "Iteration 10, loss = 0.14635416\n",
            "Validation score: 0.887833\n",
            "Iteration 11, loss = 0.13654782\n",
            "Validation score: 0.891500\n",
            "Iteration 12, loss = 0.12984302\n",
            "Validation score: 0.892500\n",
            "Iteration 13, loss = 0.11400859\n",
            "Validation score: 0.892167\n",
            "Iteration 14, loss = 0.11108869\n",
            "Validation score: 0.897000\n",
            "Iteration 15, loss = 0.09853518\n",
            "Validation score: 0.885000\n",
            "Iteration 16, loss = 0.09247732\n",
            "Validation score: 0.890333\n",
            "Iteration 17, loss = 0.08379185\n",
            "Validation score: 0.885833\n",
            "Iteration 18, loss = 0.06991661\n",
            "Validation score: 0.890833\n",
            "Iteration 19, loss = 0.07244225\n",
            "Validation score: 0.886333\n",
            "Iteration 20, loss = 0.06663703\n",
            "Validation score: 0.892167\n",
            "Iteration 21, loss = 0.07479953\n",
            "Validation score: 0.893833\n",
            "Iteration 22, loss = 0.05852473\n",
            "Validation score: 0.890167\n",
            "Iteration 23, loss = 0.06263375\n",
            "Validation score: 0.892667\n",
            "Iteration 24, loss = 0.05100060\n",
            "Validation score: 0.893667\n",
            "Iteration 25, loss = 0.04192138\n",
            "Validation score: 0.893500\n",
            "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Baseline Accuracy: 0.8894\n",
            "Training Time: 151.33 seconds\n",
            "Confusion Matrix:\n",
            " [[837   2  19  21   4   0 110   0   6   1]\n",
            " [  3 978   2  13   1   0   2   0   1   0]\n",
            " [ 15   1 837  14  76   1  56   0   0   0]\n",
            " [ 16   6   8 911  29   0  29   0   1   0]\n",
            " [  1   0 101  30 813   0  54   0   1   0]\n",
            " [  0   0   0   0   0 954   0  27   2  17]\n",
            " [125   1  91  28  73   0 676   0   6   0]\n",
            " [  0   0   0   0   0   8   0 958   1  33]\n",
            " [  6   1   6   9   2   6   7   3 960   0]\n",
            " [  0   0   0   0   0   9   1  20   0 970]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Layer Notes:\n",
        "---\n",
        "*   Baseline Model:\n",
        "Decent improvement. 50 iterations did not converge, Accuracy = .8808, Training Time = 117.15 seconds. With early stopping: Accuracy = .8832, Training Time = 58.39 seconds.\n",
        "*   Two Layers: Loss seemed to be converging at around 0.03 when 50 iterations completed, which took 306.88 seconds. Accuracy = .8832. Confusion matrix much better than in single layer case. With early stopping: Accuracy = .8894, Training Time = 130.20 seconds.\n",
        "*   Three Layers: Loss once again seemed to be converging around 0.03 after 50 iterations, which took 700.83 seconds (around 12 minutes). Accuracy = 0.8873. Confusion matrix nearing ideal. With early stopping: Accuracy = .8883, Training Time = 254.61 seconds. Overall, not significantly better than two layers for almost double runtime.\n",
        "---\n",
        "Notes on Activation Functions (Ran on Two-Layer w/ Early Stopping):\n",
        "---\n",
        "*   Relu: Accuracy = .8894, Training Time = 130.20 seconds\n",
        "*   Logistic: Accuracy = .8895, Training Time = 148.72 seconds\n",
        "*   Identity: Accuracy = .8381, Training Time = 116.75 seconds\n",
        "*   Tanh: Accuracy = .8854, Training Time = 213.36 seconds\n",
        "---\n",
        "Notes on Optimization Method (Two-Layer, Early Stopping, Relu)\n",
        "---\n",
        "*   ADAM: Accuracy = .8894, Training Time = 130.20 seconds\n",
        "*   SGD: Accuracy = .8828, Training Time = 347.90 seconds\n",
        "*   SGD w/ learning rate adjustments: Accuracy = .8894, Training Time = 151.33 seconds\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OGqFd8FJiihH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Fashion-MNIST image classification  using pytorch"
      ],
      "metadata": {
        "id": "a2qcKggmIH8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load Fashion-MNIST\n",
        "# Classes (0-9): T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# scale to [0,1], add channel dimension -> (N, 1, 28, 28)\n",
        "X_train = (X_train.astype(\"float32\") / 255.0)[:, None, :, :]\n",
        "X_test  = (X_test.astype(\"float32\")  / 255.0)[:,  None, :, :]\n",
        "\n",
        "y_train = y_train.astype(np.int64)\n",
        "y_test  = y_test.astype(np.int64)\n",
        "\n",
        "# train/val split: last 10k of train as validation\n",
        "X_tr, X_val = X_train[:50000], X_train[50000:]\n",
        "y_tr, y_val = y_train[:50000], y_train[50000:]\n",
        "\n",
        "# wrap in PyTorch TensorDatasets and DataLoaders\n",
        "train_ds = TensorDataset(torch.from_numpy(X_tr),  torch.from_numpy(y_tr))\n",
        "val_ds   = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
        "test_ds  = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=256, shuffle=False)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=256, shuffle=False)"
      ],
      "metadata": {
        "id": "B9IQwhgcIVOl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "# In colab, you should ``change runtime type'' to GPU.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Feedforward (MLP) Model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(28*28, 256)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.relu1(self.fc1(x))\n",
        "        x = self.relu2(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "# CNN Model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(128 * 7 * 7, 256)\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.relu(self.conv3(x))\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.dropout(self.relu(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "# Training\n",
        "def train_model(model, train_loader, val_loader, optimizer, criterion, epochs=10):\n",
        "    model.to(device)\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * X_batch.size(0)\n",
        "\n",
        "        val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
        "        print(f\"Iteration {epoch+1}, \" + f\"Train Loss: {running_loss/len(train_loader.dataset):.4f} \")\n",
        "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "        print(f\"Validation score: {val_acc:.4f}\")\n",
        "\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    correct, total, loss_total = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss_total += loss.item() * X_batch.size(0)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == y_batch).sum().item()\n",
        "            total += y_batch.size(0)\n",
        "    return loss_total / total, correct / total\n",
        "\n",
        "def get_all_preds(model, loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            outputs = model(X_batch)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            all_preds.append(preds.cpu().numpy())\n",
        "            all_labels.append(y_batch.cpu().numpy())\n",
        "    return np.concatenate(all_labels), np.concatenate(all_preds)\n",
        "\n",
        "model = CNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "start_time = time.time()\n",
        "train_model(model, train_loader, val_loader, optimizer, criterion, epochs=10)\n",
        "train_time = time.time() - start_time\n",
        "\n",
        "y_true, y_pred = get_all_preds(model, test_loader)\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "\n",
        "print(f\"Training Time: {train_time:.2f} seconds\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0REsDBunNmEl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa37d493-51d0-4aa7-f40f-f533f23b85ae"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Iteration 1, Train Loss: 0.5984 \n",
            "Validation Loss: 0.3682\n",
            "Validation score: 0.8648\n",
            "Iteration 2, Train Loss: 0.3615 \n",
            "Validation Loss: 0.2985\n",
            "Validation score: 0.8869\n",
            "Iteration 3, Train Loss: 0.3084 \n",
            "Validation Loss: 0.2691\n",
            "Validation score: 0.8989\n",
            "Iteration 4, Train Loss: 0.2716 \n",
            "Validation Loss: 0.2486\n",
            "Validation score: 0.9067\n",
            "Iteration 5, Train Loss: 0.2449 \n",
            "Validation Loss: 0.2361\n",
            "Validation score: 0.9125\n",
            "Iteration 6, Train Loss: 0.2273 \n",
            "Validation Loss: 0.2221\n",
            "Validation score: 0.9177\n",
            "Iteration 7, Train Loss: 0.2054 \n",
            "Validation Loss: 0.2231\n",
            "Validation score: 0.9171\n",
            "Iteration 8, Train Loss: 0.1912 \n",
            "Validation Loss: 0.2109\n",
            "Validation score: 0.9235\n",
            "Iteration 9, Train Loss: 0.1758 \n",
            "Validation Loss: 0.2167\n",
            "Validation score: 0.9210\n",
            "Iteration 10, Train Loss: 0.1615 \n",
            "Validation Loss: 0.2123\n",
            "Validation score: 0.9231\n",
            "Confusion Matrix:\n",
            " [[882   0  11  12   3   1  88   0   3   0]\n",
            " [  1 978   0  15   2   0   4   0   0   0]\n",
            " [ 18   1 884   7  47   0  41   0   2   0]\n",
            " [  8   0  10 927  26   0  28   0   1   0]\n",
            " [  0   1  39  15 875   0  70   0   0   0]\n",
            " [  0   0   0   0   0 981   0  15   0   4]\n",
            " [ 98   1  62  24  51   0 760   0   4   0]\n",
            " [  0   0   0   0   0   4   0 974   0  22]\n",
            " [  5   0   0   2   1   1   1   2 988   0]\n",
            " [  0   0   1   0   0   3   0  29   0 967]]\n",
            "Training Time: 24.87 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes (Baseline MLP Model):\n",
        "---\n",
        "*   Final accuracy: .8864, Training Time = 11.32 seconds (way faster than sklearn!)\n",
        "---\n",
        "Notes (CNN Model):\n",
        "---\n",
        "*   Two-layer: Final accuracy: .9175, Training Time = 22.10 seconds.\n",
        "*   Three-layer: Final accuracy: .9231, Training Time = 24.87 seconds.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JOkTaqNdnmpD"
      }
    }
  ]
}